{"id":217,"date":"2015-12-08T17:23:30","date_gmt":"2015-12-08T17:23:30","guid":{"rendered":"http:\/\/reproducibility.stanford.edu\/?p=217"},"modified":"2016-04-06T15:28:43","modified_gmt":"2016-04-06T15:28:43","slug":"big-problems-for-common-fmri-thresholding-methods","status":"publish","type":"post","link":"https:\/\/reproducibility.stanford.edu\/big-problems-for-common-fmri-thresholding-methods\/","title":{"rendered":"Big problems for common fMRI thresholding methods"},"content":{"rendered":"<p><span style=\"font-weight: 400;\">A <a href=\"http:\/\/arxiv.org\/abs\/1511.01863\">new preprint<\/a> has been posted to the ArXiv that has very important implications and should be required reading for all fMRI researchers. \u00a0Anders Eklund, Tom Nichols, and Hans Knutsson applied task fMRI analyses to a large number of resting fMRI datasets, in order to identify the empirical corrected \u201cfamilywise\u201d Type I error rates observed under the null hypothesis for both voxel-wise and cluster-wise inference. \u00a0What they found is shocking: While voxel-wise error rates were valid, nearly all cluster-based parametric methods (except for FSL\u2019s FLAME 1) have greatly inflated familywise Type I error rates. \u00a0This inflation was worst for analyses using lower cluster-forming thresholds (e.g. p=0.01) compared to higher thresholds, but even with higher thresholds there was serious inflation. \u00a0This should be a sobering wake-up call for fMRI researchers, as it suggests that the methods used in a large number of previous publications suffer from exceedingly high false positive rates (sometimes greater than 50%). \u00a0<\/span><\/p>\n<figure id=\"attachment_220\" aria-describedby=\"caption-attachment-220\" style=\"width: 600px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_figure1.png\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-220\" src=\"http:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_figure1-1024x836.png\" alt=\"eklund_figure1\" width=\"600\" height=\"490\" srcset=\"https:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_figure1-1024x836.png 1024w, https:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_figure1-300x245.png 300w, https:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_figure1.png 1360w\" sizes=\"auto, (max-width: 600px) 100vw, 600px\" \/><\/a><figcaption id=\"caption-attachment-220\" class=\"wp-caption-text\">Figure 1 from Eklund et al., showing substantial inflation of familywise error rates for most common cluster-based thresholding methods.<\/figcaption><\/figure>\n<p><span style=\"font-weight: 400;\">They also examined the commonly used heuristic correction (what they call \u201cad-hoc cluster inference\u201d) of p=0.001 and a cluster extent threshold of 80 mm^3. \u00a0This method showed a shockingly high rate of false positives, up to 90% familywise error in some cases. \u00a0Hopefully this paper will serve as the deathknell for such heuristic corrections.<\/span><\/p>\n<figure id=\"attachment_219\" aria-describedby=\"caption-attachment-219\" style=\"width: 400px\" class=\"wp-caption aligncenter\"><a href=\"http:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_fig7.png\"><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-219\" src=\"http:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_fig7-300x230.png\" alt=\"eklund_fig7\" width=\"400\" height=\"307\" srcset=\"https:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_fig7-300x230.png 300w, https:\/\/reproducibility.stanford.edu\/wp-content\/uploads\/2015\/12\/eklund_fig7.png 690w\" sizes=\"auto, (max-width: 400px) 100vw, 400px\" \/><\/a><figcaption id=\"caption-attachment-219\" class=\"wp-caption-text\">Figure 7 from Eklund et al., showing massive inflation of familywise Type I error rate using ad-hoc clustering inference.<\/figcaption><\/figure>\n<p><span style=\"font-weight: 400;\">Another set of serious concerns were raised about the simulation-based methods implemented in AFNI\u2019s 3DclustSim tool:<\/span><\/p>\n<blockquote><p><span style=\"font-weight: 400;\">Firstly, AFNI estimates the spatial group smoothness differently compared to SPM and FSL. AFNI averages smoothness estimates from the first level analysis, whereas SPM and FSL estimate the group smoothness using the group residuals from the general linear model [39]. The group smoothness used by 3dClustSim may for this reason be too low (compared to SPM and FSL, see Figure 10); the variation of smoothness over subjects is not considered. Secondly, a 15 year old bug was found in 3dClustSim while testing the three software packages (the bug was fixed by the AFNI group as of May 2015, during preparation of this manuscript). The effect of the bug was an underestimation of how likely it is to find a cluster of a certain size (in other words, the p-values reported by 3dClustSim were too low).<\/span><\/p><\/blockquote>\n<p><span style=\"font-weight: 400;\">I have been disturbed in recent years to see an increasing number of papers that perform most of their analyses using FSL or SPM, but then use the AFNI tool for multiple comparison correction. \u00a0It is now clear why this tool was so attractive to so many researchers: It systematically undercorrects for multiple comparisons, leading to \u201cbetter results\u201d (i.e. inflated false positives). \u00a0As an aside, this also points to the critical need for reporting of software versions in empirical papers; without this, it is impossible to know whether results obtained using AFNI suffered from this bug or not. \u00a0<\/span><\/p>\n<p><span style=\"font-weight: 400;\">To the degree that there is a star of the Eklund et al. paper, it was nonparametric testing using permutation tests (for example, as implemented in the FSL randomise tool), though even those methods did not escape unscathed. \u00a0The standard objection to nonparametric testing has been the substantial computational load of these methods. However, Eklund et al. also raise concerns about their ability to control Type I error rate, particularly in the one-sample t-test case (as opposed to the two-sample group comparison case where it did well). \u00a0It appears that the assumption of symmetrically distributed errors required by the one-sample permutation approach may be violated, leading to inflated error, though these violations appear to be relatively minor compared to those seen with the parametric methods.<\/span><\/p>\n<p><span style=\"font-weight: 400;\">What are we to take away from this? \u00a0First, the results clearly show that cluster-based inference with traditional parametric tools should always use cluster-forming thresholds no less stringent than p&lt;0.001, as error inflation was much worse for less stringent cluster-forming thresholds. \u00a0Second, the results strongly suggest that permutation testing is likely the best approach to prevent false positives without being overly stringent (e.g., using voxelwise inference rather that cluster inference). The platform being developed by the Center for Reproducible Neuroscience should make this much easier for researchers to apply through the use of high-performance computing. \u00a0If one must use a parametric method, then FSL\u2019s FLAME 1 with a cluster-forming threshold appears to be the best bet, though it is sometimes quite conservative. \u00a0Third, the results should cause substantial suspicion about the use of AFNI\u2019s 3DClustSim tool, as even the new version with the bug fix doesn\u2019t bring false positive rates into line. \u00a0<\/span><br \/>\n<span style=\"font-weight: 400;\">More generally, the fact that it took our field more than twenty years to discover that some of our most common methods are badly flawed is bracing. \u00a0At the same time, it is only through the availability of massive\u00a0open data repositories\u00a0(including <a href=\"http:\/\/www.nitrc.org\/projects\/fcon_1000\">1000 Functional Connectomes<\/a> and <a href=\"http:\/\/www.openfmri.org\">OpenfMRI<\/a>) that this kind of analysis\u00a0could be done. \u00a0We need much more work of the kind presented in the Eklund et al. paper in order to better understand how we can ultimately ensure that the results of fMRI studies are reproducible.\u00a0<\/span><\/p>\n","protected":false},"excerpt":{"rendered":"<p>A new preprint has been posted to the ArXiv that has very important implications and should be required reading for all fMRI researchers. \u00a0Anders Eklund, Tom Nichols, and Hans Knutsson applied task fMRI analyses to a large number of resting fMRI datasets, in order to identify the empirical corrected \u201cfamilywise\u201d Type I error rates observed [&hellip;]<\/p>\n","protected":false},"author":2,"featured_media":0,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"_exactmetrics_skip_tracking":false,"_exactmetrics_sitenote_active":false,"_exactmetrics_sitenote_note":"","_exactmetrics_sitenote_category":0,"footnotes":""},"categories":[6],"tags":[],"class_list":["post-217","post","type-post","status-publish","format-standard","hentry","category-blog"],"_links":{"self":[{"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/posts\/217","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/users\/2"}],"replies":[{"embeddable":true,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/comments?post=217"}],"version-history":[{"count":6,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/posts\/217\/revisions"}],"predecessor-version":[{"id":227,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/posts\/217\/revisions\/227"}],"wp:attachment":[{"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/media?parent=217"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/categories?post=217"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/reproducibility.stanford.edu\/wp-json\/wp\/v2\/tags?post=217"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}